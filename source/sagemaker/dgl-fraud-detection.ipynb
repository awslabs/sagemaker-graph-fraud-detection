{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Fraud Detection with DGL on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "\n",
    "The dataset we use is a typical example of financial transactions dataset that many companies have. The dataset consists of three tables:\n",
    "\n",
    "* **Relations/Transactions**: Records links or actions between two users. \n",
    "* **User Features**: demographic features of each user\n",
    "\n",
    "Now let's upload the raw data to a convenient location in S3 where it will be picked up by the preprocessing job and training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data/ 'https://linqs-data.soe.ucsc.edu/public/social_spammer/usersdata.csv.gz'\n",
    "!wget -P data/ 'https://linqs-data.soe.ucsc.edu/public/social_spammer/relations.csv.gz'\n",
    "\n",
    "from sagemaker.s3 import S3Uploader\n",
    "bucket = 'SAGEMAKER_S3_BUCKET'\n",
    "prefix = 'dgl'\n",
    "\n",
    "input_data = 's3://{}/{}/raw-data'.format(bucket, prefix)\n",
    "S3Uploader.upload('data/usersdata.csv.gz', input_data)\n",
    "S3Uploader.upload('data/relations.csv.gz', input_data)\n",
    "\n",
    "train_data = 's3://{}/{}/processed-data'.format(bucket, prefix)\n",
    "train_output = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container for Preprocessing and Feature Engineering\n",
    "\n",
    "Data preprocessing and feature engineering is an important component of the ML lifecycle, and Amazon SageMaker Processing allows you to do these easily on a managed infrastructure. Now, we'll create a lightweight container that will serve as the environment for our data preprocessing. The container can also be easily customized to add in more dependencies if our preprocessing job requires it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'sagemaker-preprocessing-container'\n",
    "ecr_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, ecr_repository)\n",
    "\n",
    "!bash data-preprocessing/container/build_and_push.sh $ecr_repository docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing job with Amazon SageMaker Processing\n",
    "\n",
    "The script we have defined at `data-preprocessing/graph_data_preprocessor.py` performs data preprocessing and feature engineering transformations on the raw data. Some of the data transformation and feature engineering techniques include:\n",
    "\n",
    "* Aggregating and encoding user activity into a hour-indexed feature vector\n",
    "* Constructing graph edgelists between users accounts for the various relation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=ecr_repository_uri,\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='SAGEMAKER_PROCESSING_INSTANCE_TYPE')\n",
    "\n",
    "script_processor.run(code='data-preprocessing/graph_data_preprocessor.py',\n",
    "                     inputs=[ProcessingInput(source=input_data,\n",
    "                                             destination='/opt/ml/processing/input')],\n",
    "                     outputs=[ProcessingOutput(destination=train_data,\n",
    "                                               source='/opt/ml/processing/output')],\n",
    "                     arguments=['--train-days', '5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results of Data Preprocessing\n",
    "\n",
    "Once the preprocessing job is complete, we can take a look at the contents of the S3 bucket to see the transformed data. We have a set of bipartite edge lists between users and different device id types as well as the user features, labels and a set of users to validate our graph model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from sagemaker.s3 import S3Downloader\n",
    "processed_files = S3Downloader.list(train_data)\n",
    "print(\"===== Processed Files =====\")\n",
    "print('\\n'.join(processed_files))\n",
    "\n",
    "# optionally download processed data\n",
    "# S3Downloader.download(train_data, train_data.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Graph Neural Network with DGL\n",
    "\n",
    "Graph Neural Networks work by learning representation for nodes or edges of a graph that are well suited for some downstream task. We can model the fraud detection problem as a node classification task, and the goal of the graph neural network would be to learn how to use information from the topology of the sub-graph for each user node to transform the user node features to a representation space where the node can be easily classified as fraud or not.\n",
    "\n",
    "Specifically, we will be using a relational graph convolutional neural network model on a heterogeneous graph since we have nodes and edges of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "To train the graph neural network, we need to define a few hyperparameters that determine:\n",
    "\n",
    "* The kind of graph we're constructing\n",
    "* The class of graph neural network models we will be using \n",
    "* The network architecture\n",
    "* The optimizer and optimization parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = \",\".join(map(lambda x: x.split(\"/\")[-1], [file for file in processed_files if \"relation\" in file]))\n",
    "params = {'nodes' : 'user_features.csv',\n",
    "          'edges': edges,\n",
    "          'labels': 'tags.csv',\n",
    "          'model': 'rgcn',\n",
    "          'num-gpus': 1,\n",
    "          'embedding-size': 64,\n",
    "          'n-layers': 2,\n",
    "          'n-epochs': 100,\n",
    "          'optimizer': 'adam',\n",
    "          'lr': 1e-2\n",
    "        }\n",
    "\n",
    "print(\"Graph will be constructed using the following edgelists:\\n{}\" .format('\\n'.join(bipartite_edges.split(\",\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit SageMaker Estimator\n",
    "\n",
    "With the hyperparameters defined, we can kick off the training job. We will be using the Deep Graph Library (DGL), with MXNet as the backend deep learning framework, to define and train the graph neural network. Amazon SageMaker makes it do this with the Framework estimators which have the deep learning frameworks already setup. Here, we create a SageMaker MXNet estimator and pass in our model training script, hyperparameters, as well as the number and type of training instances we want.\n",
    "\n",
    "We can then `fit` the estimator on the the training data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "estimator = MXNet(entry_point='train_dgl_entry_point.py',\n",
    "                  source_dir='dgl-fraud-detection',\n",
    "                  role=role, \n",
    "                  train_instance_count=1, \n",
    "                  train_instance_type='SAGEMAKER_TRAINING_INSTANCE_TYPE',\n",
    "                  framework_version=\"1.4.1\",\n",
    "                  py_version='py3',\n",
    "                  hyperparameters=params,\n",
    "                  output_path=train_output,\n",
    "                  code_location=train_output,\n",
    "                  sagemaker_session=sess)\n",
    "\n",
    "estimator.fit({'train': train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is completed, the training instances are automatically saved and SageMaker stores the trained model and evaluation results to a location in S3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
