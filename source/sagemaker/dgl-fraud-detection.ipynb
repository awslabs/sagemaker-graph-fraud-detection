{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Fraud Detection with DGL on Amazon SageMaker\n",
    "\n",
    "This notebook shows an end to end pipeline to train a fraud detection model using graph neural networks. \n",
    "\n",
    "First, we process the raw dataset to prepare the features and extract the interactions in the dataset that will be used to construct the graph. \n",
    "\n",
    "Then, we create a launch a training job using the SageMaker framework estimator to train a graph neural network model with DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash setup.sh\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker_graph_fraud_detection import config, container_build\n",
    "\n",
    "role = config.role\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "\n",
    "The dataset we use is the [IEEE-CIS Fraud Detection dataset](https://www.kaggle.com/c/ieee-fraud-detection/data?select=train_transaction.csv) which is a typical example of financial transactions dataset that many companies have. The dataset consists of two tables:\n",
    "\n",
    "* **Transactions**: Records transactions and metadata about transactions between two users. Examples of columns include the product code for the transaction and features on the card used for the transaction. \n",
    "* **Identity**: Contains information about the identity users performing transactions. Examples of columns here include the device type and device ids used.\n",
    "\n",
    "We will go over the specific data schema in subsequent cells but now let's move the raw data to a convenient location in the S3 bucket for this proejct, where it will be picked up by the preprocessing job and training job.\n",
    "\n",
    "If you would like to use your own dataset for this demonstration. Replace the `raw_data_location` with the s3 path or local path of your dataset, and modify the data preprocessing step as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with an S3 location or local path to point to your own dataset\n",
    "raw_data_location = 's3://sagemaker-solutions-us-west-2/Fraud-detection-in-financial-networks/data'\n",
    "\n",
    "session_prefix = 'dgl-fraud-detection'\n",
    "input_data = 's3://{}/{}/{}'.format(config.solution_bucket, session_prefix, config.s3_data_prefix)\n",
    "\n",
    "!aws s3 cp --recursive $raw_data_location $input_data\n",
    "\n",
    "# Set S3 locations to store processed data for training and post-training results and artifacts respectively\n",
    "train_data = 's3://{}/{}/{}'.format(config.solution_bucket, session_prefix, config.s3_processing_output)\n",
    "train_output = 's3://{}/{}/{}'.format(config.solution_bucket, session_prefix, config.s3_train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container for Preprocessing and Feature Engineering\n",
    "\n",
    "Data preprocessing and feature engineering is an important component of the ML lifecycle, and Amazon SageMaker Processing allows you to do these easily on a managed infrastructure. First, we'll create a lightweight container that will serve as the environment for our data preprocessing. \n",
    "\n",
    "The Dockerfile that defines the container is shown below and it only contains the pandas package as a dependency but it can also be easily customized to add in more dependencies if your data preprocessing job requires it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize data-preprocessing/container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run a simple script to build a container image using the Dockerfile, and push the image to Amazon ECR. The container image will have a unique URI which the SageMaker Processing job executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = config.region_name\n",
    "account_id = config.aws_account\n",
    "ecr_repository = config.ecr_repository\n",
    "\n",
    "if config.container_build_project == \"local\":\n",
    "    !cd  data-preprocessing && bash container/build_and_push.sh $ecr_repository $region $account_id\n",
    "else:\n",
    "    container_build.build(config.container_build_project)\n",
    "ecr_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, ecr_repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing job with Amazon SageMaker Processing\n",
    "\n",
    "The script we have defined at `data-preprocessing/graph_data_preprocessor.py` performs data preprocessing and feature engineering transformations on the raw data. We provide a general processing framework to convert a relational table to heterogeneous graph edgelists based on the column types of the relational table. Some of the data transformation and feature engineering techniques include:\n",
    "\n",
    "* Performing numerical encoding for categorical variables and logarithmic transformation for transaction amount\n",
    "* Constructing graph edgelists between transactions and other entities for the various relation types\n",
    "\n",
    "The inputs to the data preprocessing script are passed in as python command line arguments. All the columns in the relational table are classifed into one of 3 types for the purposes of data transformation: \n",
    "\n",
    "* **Identity columns** `--id-cols`: columns that contain identity information related to a user or transaction for example IP address, Phone Number, device identifiers etc. These column types become node types in the heterogeneous graph, and the entries in these columns become the nodes. The column names for these column types need to passed in to the script.\n",
    "\n",
    "* **Categorical columns** `--cat-cols`: columns that correspond to categorical features for a user's age group or whether a provided address matches with an address on file. The entries in these columns undergo numerical feature transformation and are used as node attributes in the heterogeneous graph. The columns names for these column types also needs to be passed in to the script\n",
    "\n",
    "* **Numerical columns**: columns that correspond to numerical features like how many times a user has tried a transaction and so on. The entries here are also used as node attributes in the heterogeneous graph. The script assumes that all columns in the tables that are not identity columns or categorical columns are numerical columns\n",
    "\n",
    "In order to adapt the preprocessing script to work with data in the same format, you can simply change the python arguments used in the cell below to a comma seperate string for the column names in your dataset. If your dataset is in a different format, then you will also have to modify the preprocessing script at `data-preprocessing/graph_data_preprocessor.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=ecr_repository_uri,\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')\n",
    "\n",
    "script_processor.run(code='data-preprocessing/graph_data_preprocessor.py',\n",
    "                     inputs=[ProcessingInput(source=input_data,\n",
    "                                             destination='/opt/ml/processing/input')],\n",
    "                     outputs=[ProcessingOutput(destination=train_data,\n",
    "                                               source='/opt/ml/processing/output')],\n",
    "                     arguments=['--id-cols', 'card1,card2,card3,card4,card5,card6,ProductCD,addr1,addr2,P_emaildomain,R_emaildomain',\n",
    "                                '--cat-cols','M1,M2,M3,M4,M5,M6,M7,M8,M9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results of Data Preprocessing\n",
    "\n",
    "Once the preprocessing job is complete, we can take a look at the contents of the S3 bucket to see the transformed data. We have a set of bipartite edge lists between transactions and different device id types as well as the features, labels and a set of transactions to validate our graph model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from sagemaker.s3 import S3Downloader\n",
    "processed_files = S3Downloader.list(train_data)\n",
    "print(\"===== Processed Files =====\")\n",
    "print('\\n'.join(processed_files))\n",
    "\n",
    "# optionally download processed data\n",
    "# S3Downloader.download(train_data, train_data.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Graph Neural Network with DGL\n",
    "\n",
    "Graph Neural Networks work by learning representation for nodes or edges of a graph that are well suited for some downstream task. We can model the fraud detection problem as a node classification task, and the goal of the graph neural network would be to learn how to use information from the topology of the sub-graph for each transaction node to transform the node's features to a representation space where the node can be easily classified as fraud or not.\n",
    "\n",
    "Specifically, we will be using a relational graph convolutional neural network model (R-GCN) on a heterogeneous graph since we have nodes and edges of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "To train the graph neural network, we need to define a few hyperparameters that determine properties such as the class of graph neural network models we will be using, the network architecture and the optimizer and optimization parameters. \n",
    "\n",
    "Here we're setting only a few of the hyperparameters, to see all the hyperparameters and their default values, see `dgl-fraud-detection/estimator_fns.py`. The parameters set below are:\n",
    "\n",
    "* **`nodes`** is the name of the file that contains the `node_id`s of the target nodes and the node features.\n",
    "* **`edges`** is a regular expression that when expanded lists all the filenames for the edgelists\n",
    "* **`labels`** is the name of the file tha contains the target `node_id`s and their labels\n",
    "* **`model`** specify which graph neural network to use, this should be set to `r-gcn`\n",
    "\n",
    "The following hyperparameters can be tuned and adjusted to improve model performance\n",
    "* **batch-size** is the number nodes that are used to compute a single forward pass of the GNN\n",
    "\n",
    "* **embedding-size** is the size of the embedding dimension for non target nodes\n",
    "* **n-neighbors** is the number of neighbours to sample for each target node during graph sampling for mini-batch training\n",
    "* **n-layers** is the number of GNN layers in the model\n",
    "* **n-epochs** is the number of training epochs for the model training job\n",
    "* **optimizer** is the optimization algorithm used for gradient based parameter updates\n",
    "* **lr** is the learning rate for parameter updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = \",\".join(map(lambda x: x.split(\"/\")[-1], [file for file in processed_files if \"relation\" in file]))\n",
    "params = {'nodes' : 'features.csv',\n",
    "          'edges': 'relation*',\n",
    "          'labels': 'tags.csv',\n",
    "          'model': 'rgcn',\n",
    "          'num-gpus': 1,\n",
    "          'batch-size': 10000,\n",
    "          'embedding-size': 64,\n",
    "          'n-neighbors': 1000,\n",
    "          'n-layers': 2,\n",
    "          'n-epochs': 10,\n",
    "          'optimizer': 'adam',\n",
    "          'lr': 1e-2\n",
    "        }\n",
    "\n",
    "print(\"Graph will be constructed using the following edgelists:\\n{}\" .format('\\n'.join(edges.split(\",\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit SageMaker Estimator\n",
    "\n",
    "With the hyperparameters defined, we can kick off the training job. We will be using the Deep Graph Library (DGL), with MXNet as the backend deep learning framework, to define and train the graph neural network. Amazon SageMaker makes it do this with the Framework estimators which have the deep learning frameworks already setup. Here, we create a SageMaker MXNet estimator and pass in our model training script, hyperparameters, as well as the number and type of training instances we want.\n",
    "\n",
    "We can then `fit` the estimator on the the training data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "from time import strftime, gmtime\n",
    "\n",
    "estimator = MXNet(entry_point='train_dgl_mxnet_entry_point.py',\n",
    "                  source_dir='sagemaker_graph_fraud_detection/dgl_fraud_detection',\n",
    "                  role=role, \n",
    "                  train_instance_count=1, \n",
    "                  train_instance_type='ml.p3.2xlarge',\n",
    "                  framework_version=\"1.6.0\",\n",
    "                  py_version='py3',\n",
    "                  hyperparameters=params,\n",
    "                  output_path=train_output,\n",
    "                  code_location=train_output,\n",
    "                  sagemaker_session=sess)\n",
    "\n",
    "training_job_name = \"{}-{}\".format(config.solution_prefix, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "estimator.fit({'train': train_data}, job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is completed, the training instances are automatically saved and SageMaker stores the trained model and evaluation results to a location in S3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}