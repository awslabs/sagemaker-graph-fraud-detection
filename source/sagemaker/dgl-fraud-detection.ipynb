{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Fraud Detection with DGL on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "\n",
    "The dataset we use is the [IEEE-CIS Fraud Detection dataset](https://www.kaggle.com/c/ieee-fraud-detection/data?select=train_transaction.csv) which is a typical example of financial transactions dataset that many companies have. The dataset consists of two tables:\n",
    "\n",
    "* **Transactions**: Records transactions and metadata about transactions between two users.\n",
    "* **Identity**: Contains information about the identity users performing transactions\n",
    "\n",
    "Now let's move the raw data to a convenient location in the S3 bucket for this proejct, where it will be picked up by the preprocessing job and training job.\n",
    "\n",
    "If you would like to use your own dataset for this demonstration. Replace the `raw_data_location` with the s3 path or local path of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with an S3 location or local path to point to your own dataset\n",
    "raw_data_location = 's3://sagemaker-solutions-us-west-2/Fraud-detection-in-financial-networks/data'\n",
    "\n",
    "bucket = 'SAGEMAKER_S3_BUCKET'\n",
    "prefix = 'dgl'\n",
    "input_data = 's3://{}/{}/raw-data'.format(bucket, prefix)\n",
    "\n",
    "!aws s3 cp --recursive $raw_data_location $input_data\n",
    "\n",
    "# Set S3 locations to store processed data for training and post-training results and artifacts respectively\n",
    "train_data = 's3://{}/{}/processed-data'.format(bucket, prefix)\n",
    "train_output = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container for Preprocessing and Feature Engineering\n",
    "\n",
    "Data preprocessing and feature engineering is an important component of the ML lifecycle, and Amazon SageMaker Processing allows you to do these easily on a managed infrastructure. Now, we'll create a lightweight container that will serve as the environment for our data preprocessing. The container can also be easily customized to add in more dependencies if our preprocessing job requires it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'sagemaker-preprocessing-container'\n",
    "ecr_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, ecr_repository)\n",
    "\n",
    "!bash data-preprocessing/container/build_and_push.sh $ecr_repository docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing job with Amazon SageMaker Processing\n",
    "\n",
    "The script we have defined at `data-preprocessing/graph_data_preprocessor.py` performs data preprocessing and feature engineering transformations on the raw data. Some of the data transformation and feature engineering techniques include:\n",
    "\n",
    "* Performing numerical encoding for categorical variables and logarithmic transformation for transaction amount\n",
    "* Constructing graph edgelists between transactions and other entities for the various relation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=ecr_repository_uri,\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='SAGEMAKER_PROCESSING_INSTANCE_TYPE')\n",
    "\n",
    "script_processor.run(code='data-preprocessing/graph_data_preprocessor.py',\n",
    "                     inputs=[ProcessingInput(source=input_data,\n",
    "                                             destination='/opt/ml/processing/input')],\n",
    "                     outputs=[ProcessingOutput(destination=train_data,\n",
    "                                               source='/opt/ml/processing/output')],\n",
    "                     arguments=['--id-cols', 'card1,card2,card3,card4,card5,card6,ProductCD,addr1,addr2,P_emaildomain,R_emaildomain',\n",
    "                                '--cat-cols','M1,M2,M3,M4,M5,M6,M7,M8,M9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results of Data Preprocessing\n",
    "\n",
    "Once the preprocessing job is complete, we can take a look at the contents of the S3 bucket to see the transformed data. We have a set of bipartite edge lists between transactions and different device id types as well as the features, labels and a set of transactions to validate our graph model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from sagemaker.s3 import S3Downloader\n",
    "processed_files = S3Downloader.list(train_data)\n",
    "print(\"===== Processed Files =====\")\n",
    "print('\\n'.join(processed_files))\n",
    "\n",
    "# optionally download processed data\n",
    "# S3Downloader.download(train_data, train_data.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Graph Neural Network with DGL\n",
    "\n",
    "Graph Neural Networks work by learning representation for nodes or edges of a graph that are well suited for some downstream task. We can model the fraud detection problem as a node classification task, and the goal of the graph neural network would be to learn how to use information from the topology of the sub-graph for each transaction node to transform the node's features to a representation space where the node can be easily classified as fraud or not.\n",
    "\n",
    "Specifically, we will be using a relational graph convolutional neural network model (R-GCN) on a heterogeneous graph since we have nodes and edges of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "To train the graph neural network, we need to define a few hyperparameters that determine:\n",
    "\n",
    "* The kind of graph we're constructing\n",
    "* The class of graph neural network models we will be using \n",
    "* The network architecture\n",
    "* The optimizer and optimization parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = \",\".join(map(lambda x: x.split(\"/\")[-1], [file for file in processed_files if \"relation\" in file]))\n",
    "params = {'nodes' : 'features.csv',\n",
    "          'edges': 'relation*',\n",
    "          'labels': 'tags.csv',\n",
    "          'model': 'rgcn',\n",
    "          'num-gpus': 1,\n",
    "          'batch-size': 10000,\n",
    "          'embedding-size': 64,\n",
    "          'n-neighbors': 1000,\n",
    "          'n-layers': 2,\n",
    "          'n-epochs': 10,\n",
    "          'optimizer': 'adam',\n",
    "          'lr': 1e-2\n",
    "        }\n",
    "\n",
    "print(\"Graph will be constructed using the following edgelists:\\n{}\" .format('\\n'.join(edges.split(\",\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit SageMaker Estimator\n",
    "\n",
    "With the hyperparameters defined, we can kick off the training job. We will be using the Deep Graph Library (DGL), with MXNet as the backend deep learning framework, to define and train the graph neural network. Amazon SageMaker makes it do this with the Framework estimators which have the deep learning frameworks already setup. Here, we create a SageMaker MXNet estimator and pass in our model training script, hyperparameters, as well as the number and type of training instances we want.\n",
    "\n",
    "We can then `fit` the estimator on the the training data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "estimator = MXNet(entry_point='train_dgl_mxnet_entry_point.py',\n",
    "                  source_dir='dgl-fraud-detection',\n",
    "                  role=role, \n",
    "                  train_instance_count=1, \n",
    "                  train_instance_type='SAGEMAKER_TRAINING_INSTANCE_TYPE',\n",
    "                  framework_version=\"1.6.0\",\n",
    "                  py_version='py3',\n",
    "                  hyperparameters=params,\n",
    "                  output_path=train_output,\n",
    "                  code_location=train_output,\n",
    "                  sagemaker_session=sess)\n",
    "\n",
    "estimator.fit({'train': train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is completed, the training instances are automatically saved and SageMaker stores the trained model and evaluation results to a location in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}